{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Linear(nn.Module):\n",
    "    def __init__(self, ch_in, ch_out):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Linear(ch_in, ch_out)\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "        \n",
    "        \n",
    "class Net(nn.Module):\n",
    "    def __init__(self, ch_in, ch_out):\n",
    "        super().__init__()\n",
    "        # 嵌套Inner类实例，属于定义网络常见的情况\n",
    "        self.linear_inner = Linear(ch_in, ch_in)\n",
    "        self.linear_outer = nn.Linear(ch_in, ch_out)\n",
    "    def forward(self, x):\n",
    "        out = self.linear_inner(x)\n",
    "        out = self.linear_outer(x)\n",
    "        return out\n",
    "    \n",
    "    def init_weights(self):\n",
    "#         print(help(self.modules))\n",
    "        # 递归获得网络的所有子代Module\n",
    "        for op in self.modules():\n",
    "            # 针对不同类型操作采用不同初始化方式\n",
    "            if isinstance(op, nn.Linear):\n",
    "                nn.init.constant_(op.weight.data, val=1)\n",
    "                nn.init.constant_(op.bias.data, val=0)\n",
    "            # 这里可以对Conv等操作进行其它方式的初始化\n",
    "            else:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method modules in module torch.nn.modules.module:\n",
      "\n",
      "modules() method of __main__.Net instance\n",
      "    Returns an iterator over all modules in the network.\n",
      "    \n",
      "    Yields:\n",
      "        Module: a module in the network\n",
      "    \n",
      "    Note:\n",
      "        Duplicate modules are returned only once. In the following\n",
      "        example, ``l`` will be returned only once.\n",
      "    \n",
      "    Example::\n",
      "    \n",
      "        >>> l = nn.Linear(2, 2)\n",
      "        >>> net = nn.Sequential(l, l)\n",
      "        >>> for idx, m in enumerate(net.modules()):\n",
      "                print(idx, '->', m)\n",
      "    \n",
      "        0 -> Sequential(\n",
      "          (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "          (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "        )\n",
      "        1 -> Linear(in_features=2, out_features=2, bias=True)\n",
      "\n",
      "None\n",
      "tensor([[1.]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "# 定义网络\n",
    "net = Net(1, 1)\n",
    "# 执行初始化函数\n",
    "net.init_weights()\n",
    "# 前向推理，获得网络输出\n",
    "x = torch.ones(size=(1, 1))\n",
    "out = net(x)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('linear_inner.conv.weight', Parameter containing:\n",
      "tensor([[1.]], requires_grad=True))\n",
      "('linear_inner.conv.bias', Parameter containing:\n",
      "tensor([0.], requires_grad=True))\n",
      "('linear_outer.weight', Parameter containing:\n",
      "tensor([[1.]], requires_grad=True))\n",
      "('linear_outer.bias', Parameter containing:\n",
      "tensor([0.], requires_grad=True))\n"
     ]
    }
   ],
   "source": [
    "# 遍历网络包含的参数，观察初始化结果\n",
    "for param in net.named_parameters():\n",
    "    print(param) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class SGD in module torch.optim.sgd:\n",
      "\n",
      "class SGD(torch.optim.optimizer.Optimizer)\n",
      " |  Implements stochastic gradient descent (optionally with momentum).\n",
      " |  \n",
      " |  Nesterov momentum is based on the formula from\n",
      " |  `On the importance of initialization and momentum in deep learning`__.\n",
      " |  \n",
      " |  Args:\n",
      " |      params (iterable): iterable of parameters to optimize or dicts defining\n",
      " |          parameter groups\n",
      " |      lr (float): learning rate\n",
      " |      momentum (float, optional): momentum factor (default: 0)\n",
      " |      weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n",
      " |      dampening (float, optional): dampening for momentum (default: 0)\n",
      " |      nesterov (bool, optional): enables Nesterov momentum (default: False)\n",
      " |  \n",
      " |  Example:\n",
      " |      >>> optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
      " |      >>> optimizer.zero_grad()\n",
      " |      >>> loss_fn(model(input), target).backward()\n",
      " |      >>> optimizer.step()\n",
      " |  \n",
      " |  __ http://www.cs.toronto.edu/%7Ehinton/absps/momentum.pdf\n",
      " |  \n",
      " |  .. note::\n",
      " |      The implementation of SGD with Momentum/Nesterov subtly differs from\n",
      " |      Sutskever et. al. and implementations in some other frameworks.\n",
      " |  \n",
      " |      Considering the specific case of Momentum, the update can be written as\n",
      " |  \n",
      " |      .. math::\n",
      " |          \\begin{aligned}\n",
      " |              v_{t+1} & = \\mu * v_{t} + g_{t+1}, \\\\\n",
      " |              p_{t+1} & = p_{t} - \\text{lr} * v_{t+1},\n",
      " |          \\end{aligned}\n",
      " |  \n",
      " |      where :math:`p`, :math:`g`, :math:`v` and :math:`\\mu` denote the \n",
      " |      parameters, gradient, velocity, and momentum respectively.\n",
      " |  \n",
      " |      This is in contrast to Sutskever et. al. and\n",
      " |      other frameworks which employ an update of the form\n",
      " |  \n",
      " |      .. math::\n",
      " |          \\begin{aligned}\n",
      " |              v_{t+1} & = \\mu * v_{t} + \\text{lr} * g_{t+1}, \\\\\n",
      " |              p_{t+1} & = p_{t} - v_{t+1}.\n",
      " |          \\end{aligned}\n",
      " |  \n",
      " |      The Nesterov version is analogously modified.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      SGD\n",
      " |      torch.optim.optimizer.Optimizer\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, params, lr=<required parameter>, momentum=0, dampening=0, weight_decay=0, nesterov=False)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  step(self, closure=None)\n",
      " |      Performs a single optimization step.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          closure (callable, optional): A closure that reevaluates the model\n",
      " |              and returns the loss.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from torch.optim.optimizer.Optimizer:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  add_param_group(self, param_group)\n",
      " |      Add a param group to the :class:`Optimizer` s `param_groups`.\n",
      " |      \n",
      " |      This can be useful when fine tuning a pre-trained network as frozen layers can be made\n",
      " |      trainable and added to the :class:`Optimizer` as training progresses.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          param_group (dict): Specifies what Tensors should be optimized along with group\n",
      " |          specific optimization options.\n",
      " |  \n",
      " |  load_state_dict(self, state_dict)\n",
      " |      Loads the optimizer state.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          state_dict (dict): optimizer state. Should be an object returned\n",
      " |              from a call to :meth:`state_dict`.\n",
      " |  \n",
      " |  state_dict(self)\n",
      " |      Returns the state of the optimizer as a :class:`dict`.\n",
      " |      \n",
      " |      It contains two entries:\n",
      " |      \n",
      " |      * state - a dict holding current optimization state. Its content\n",
      " |          differs between optimizer classes.\n",
      " |      * param_groups - a dict containing all parameter groups\n",
      " |  \n",
      " |  zero_grad(self)\n",
      " |      Clears the gradients of all optimized :class:`torch.Tensor` s.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from torch.optim.optimizer.Optimizer:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import torch.optim\n",
    "print(help(torch.optim.SGD))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
